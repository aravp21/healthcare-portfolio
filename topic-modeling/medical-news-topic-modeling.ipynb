{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO25x159JJArmeP9iHc0tud"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%%capture\n","!pip install pyLDAvis\n","!pip install --upgrade ipykernel\n","!pip install -U numpy\n","\n","!pip install multi_rake\n","!pip install summa\n","!pip install keybert"],"metadata":{"id":"YWv_te9Q08_Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Web Data Scraping"],"metadata":{"id":"uhaL8XBbYc_W"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","import pandas as pd\n","\n","from datetime import datetime\n","import pytz"],"metadata":{"id":"ny6kBMqHiA1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set date to pull corresponding day's news\n","now = datetime.now(pytz.timezone('America/New_York'))\n","today = now.day\n","yesterday = today - 1\n","yesterday2 = today - 2\n","\n","if yesterday == 0 and now.month in [1, 3, 5, 7, 8, 10, 12]: \n","  yesterday = 31\n","  yesterday2 = 30\n","elif yesterday2 == 0 and now.month in [1, 3, 5, 7, 8, 10, 12]:\n","  yesterday2 = 31\n","\n","elif yesterday == 0 and now.month in [4, 6, 9, 11]: \n","  yesterday = 30\n","  yesterday2 = 29\n","elif yesterday2 == 0 and now.month in [4, 6, 9, 11]:\n","  yesterday2 = 30\n","\n","elif yesterday == 0 and now.month == 2:\n","  if now.year % 4 == 0:\n","    yesterday = 29\n","    yesterday2 = 28\n","  else:\n","    yesterday = 28\n","    yesterday2 = 27\n","elif yesterday2 == 0 and now.month == 2:\n","  if now.year % 4 == 0:\n","    yesterday2 = 29\n","  else:\n","    yesterday2 = 28"],"metadata":{"id":"b3H-Pz8YR6AM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create DataFrame with web scraped data from news sources\n","def get_news(last3, dates):\n","  df = pd.concat([get_abc(last3, dates), get_mnt(last3), get_ca(last3), get_forbes(last3), get_healthline(last3), get_evhealth(last3), get_modhealth(last3), get_scidaily(last3), \n","                  get_cnn(last3), get_atlantic(last3), get_time(last3), get_reuters()])\n","  return df"],"metadata":{"id":"IzHHlpa7sb4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_abc(text):\n","  meta_l = []\n","  meta = ''\n","\n","  spl_text = str(text.find('a', class_='AnchorLink')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('a', class_='AnchorLink').text)\n","  meta_l.append(text.find('div', class_='ContentRoll__Desc').text)\n","  return meta_l\n","\n","def get_abc(last3=False, dates=[]):\n","  page = requests.get('https://abcnews.go.com/Health/')\n","  soup = BeautifulSoup(page.content, \"html.parser\") \n","  r1 = soup.find('section', class_='ContentRoll')\n","  r2 = r1.find_all('section', 'ContentRoll__Item')\n","\n","  today = dates[0]\n","  yesterday = dates[1]\n","  yesterday2 = dates[2]\n","  if today < 10:\n","    today = f'0{today}'\n","  if yesterday < 10:\n","    yesterday = f'0{yesterday}'\n","  if yesterday2 < 10:\n","    yesterday2 = f'0{yesterday2}'\n","\n","  l = []\n","  for i in range(len(r2)):\n","    d = {}\n","    timestamp = r2[i].find('div', class_='ContentRoll__TimeStamp').text\n","\n","    if last3:\n","      if (str(today) not in timestamp and 'hour' not in timestamp) and str(yesterday) not in timestamp and str(yesterday2) not in timestamp:\n","        break\n","    else:\n","      if str(today) not in timestamp and 'hour' not in timestamp:\n","        break\n","      \n","    meta_l = scrape_abc(r2[i])\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'ABC'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"id":"NjI52Wj6TUJX","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_mnt(text):\n","  meta_l = []\n","  meta = 'https://www.medicalnewstoday.com/'\n","\n","  spl_text = str(text.find('a', class_='css-2fdibo')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('h2', class_='css-5dw8ta').text)\n","  meta_l.append(text.find('a', class_='css-2fdibo').text)\n","  return meta_l\n","\n","def get_mnt(last3=False):\n","  page = requests.get('https://www.medicalnewstoday.com/news')\n","  soup = BeautifulSoup(page.content, \"html.parser\") \n","  r1 = soup.find('ol', class_='css-1iruc8t')\n","  r2 = r1.find_all('li', 'css-kbq0t')\n","\n","  l = []\n","  for i in range(len(r2)):\n","    d = {}\n","    timestamp = r2[i].find('div', class_='css-3be604').text\n","    if last3:\n","      if ((str(today) + ', ') not in timestamp and 'hour' not in timestamp) and (str(yesterday) + ', ') not in timestamp and (str(yesterday2) + ', ') not in timestamp:\n","        break\n","    else:\n","      if (str(today) + ',') not in timestamp:\n","        break\n","\n","    meta_l = scrape_mnt(r2[i])\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'MedicalNewsToday'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"id":"qmFHgL9z2UE3","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_ca(text):\n","  meta_l = []\n","  meta = ''\n","\n","  spl_text = str(text.find('h3', class_='title')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('h3', class_='title').text.strip())\n","  meta_l.append(text.find('div', class_='content').text.strip())\n","  return meta_l\n","\n","def get_ca(last3=False):\n","  HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n","  page = requests.get('https://www.clinicaladvisor.com/home/news/', headers=HEADERS)\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find('div', class_='hm-gutenberg-block hm-content-block -dynamic')\n","  r2 = r1.find_all('article')   \n","\n","  l = []\n","  for i in range(len(r2)):\n","    d = {}\n","    timestamp = r2[i].find('time', class_='post-time -published').text\n","    if last3:\n","      if ((str(today) + ', ') not in timestamp and 'hour' not in timestamp) and (str(yesterday) + ', ') not in timestamp and (str(yesterday2) + ', ') not in timestamp:\n","        break\n","    else:\n","      if (str(today) + ',') not in timestamp:\n","        break\n","\n","    meta_l = scrape_ca(r2[i])\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'ClinicalAdvisor'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"id":"wPDg7D_8MpTn","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_forbes(text):\n","  meta_l = []\n","  meta = ''\n","\n","  spl_text = str(text.find('a', class_='_5ncu0TWl')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('a', '_5ncu0TWl').text)\n","  meta_l.append(text.find('p', 'A7hAxSNa').text)\n","  return meta_l\n","\n","def get_forbes(last3=False):\n","  page = requests.get('https://www.forbes.com/healthcare/?sh=5af135ef3b4e')\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find('div', class_='COe59')\n","  r2 = r1.find_all('div', class_='B6j66vzQ')\n","  \n","  l = []\n","  for i in range(len(r2)):\n","    d = {}\n","    timestamp = r2[i].find('span', class_='_9u4PrQql JJ0p5dnD').text\n","    if last3:\n","      if ('hour' not in timestamp and 'minute' not in timestamp) and (str(yesterday) + ', ') not in timestamp and (str(yesterday2) + ', ') not in timestamp:\n","        break\n","    else:\n","      if 'hour' not in timestamp and 'minute' not in timestamp:\n","        break\n","    \n","    meta_l = scrape_forbes(r2[i])\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'Forbes'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"id":"oEAnM-4XQrQ2","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_healthline(text):\n","  meta_l = []\n","  meta = 'https://www.healthline.com'\n","\n","  spl_text = str(text.find('a', class_='css-2fdibo')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('h2', 'css-1jcjjjn').text)\n","  meta_l.append(text.find('a', 'css-2fdibo').text)\n","  return meta_l\n","\n","def get_healthline(last3=False):\n","  page = requests.get('https://www.healthline.com/health-news')\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find('ol', class_='css-1iruc8t')\n","  r2 = r1.find_all('li', class_='css-18vzruc')\n","\n","  l = []\n","  for i in range(len(r2)):\n","    d = {}\n","    timestamp = r2[i].find('div', class_='css-mmjpxh').text\n","    if last3:\n","      if (str(today) + ',') not in timestamp and (str(yesterday) + ', ') not in timestamp and (str(yesterday2) + ', ') not in timestamp:\n","        break\n","    else:\n","      if (str(today) + ',') not in timestamp:\n","        break\n","\n","    meta_l = scrape_healthline(r2[i])\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    if not meta_l[2][-1].isalnum():\n","      d['Description'] = meta_l[2][:-1]\n","    else:\n","      d['Description'] = meta_l[2]\n","    d['Source'] = 'Healthline'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"cellView":"form","id":"2vkvvhJQrpql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_evhealth(text):\n","  meta_l = []\n","  meta = ''\n","\n","  spl_text = str(text.find('a', class_='cr-anchor')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('a', 'cr-anchor').text)\n","  meta_l.append(text.find('div', 'category-index-article__dek').text)\n","  return meta_l\n","\n","def get_evhealth(last3=False):\n","  page = requests.get('https://www.everydayhealth.com/news/')\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find('section', class_='category-articles')\n","  r2 = r1.find('article', class_='category-index-article category-index-article--large')\n","  r3 = r1.find_all('article', class_='category-index-article category-index-article--regular')\n","\n","  timestamp = r2.find('span', class_='category-index-article__date').text\n","  l = []\n","  if (str(today) + ',') in timestamp or (str(yesterday) + ', ') not in timestamp or (str(yesterday2) + ', ') not in timestamp:\n","    d = {}\n","    meta_l = scrape_evhealth(r2)\n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'EverydayHealth'\n","    l.append(d)\n","\n","    for i in range(len(r3)):\n","      d = {}\n","      timestamp = r3[i].find('span', class_='category-index-article__date').text\n","      if last3:\n","        if (str(today) + ',') not in timestamp and (str(yesterday) + ', ') not in timestamp and (str(yesterday2) + ', ') not in timestamp:\n","          break\n","      else:\n","        if (str(today) + ',') not in timestamp:\n","          break\n","\n","      meta_l = scrape_evhealth(r3[i])\n","      \n","      d['URL'] = meta_l[0]\n","      d['Headline'] = meta_l[1]\n","      d['Description'] = meta_l[2]\n","      d['Source'] = 'EverydayHealth'\n","      l.append(d)\n","      d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"cellView":"form","id":"sWUaVxgqv7wG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_modhealth(text):\n","  meta_l = []\n","  meta = ''\n","\n","  spl_text = str(text.find('h3', class_='middle-article-headline')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('h3', class_='middle-article-headline').text.strip())\n","  meta_l.append(text.find('div', class_='feature-article-summary pparagraph').text.strip())\n","  return meta_l\n","\n","def get_modhealth(last3=False):\n","  page = requests.get('https://www.modernhealthcare.com/')\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find('div', class_='block-region-content-left')\n","  r2 = r1.find('div', class_='feature-article-headline-wrapper col-md-5 col-lg-5 col-xs-12 col-sm-12')\n","  r3 = r1.find_all('div', class_='middle-article-block')\n","\n","  url = 'https://www.modernhealthcare.com'\n","  spl_text = str(r2.find('div', class_='feature-article-headline')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    url += spl_text[i]\n","\n","  l = []\n","  d = {}\n","  d['URL'] = url\n","  d['Headline'] = r2.find('div', class_='feature-article-headline').text.strip()\n","  d['Description'] = r2.find('div', class_='feature-article-summary').text.strip()\n","  d['Source'] = 'ModernHealthcare'\n","  l.append(d)\n","\n","  if last3:\n","    limit = 10\n","  else: \n","    limit = 4\n","  for i in range(limit):\n","    d = {}\n","    meta_l = scrape_modhealth(r3[i])\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'ModernHealthcare'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"id":"ghiROrro2Fnn","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_scidaily(text):\n","  meta_l = []\n","  meta = 'https://www.sciencedaily.com/'\n","\n","  spl_text = str(text.find('h3', class_='latest-head')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('h3', class_='latest-head').text.strip())\n","  meta_l.append(text.find('div', class_='latest-summary').text.split('â€”')[1].strip())\n","  return meta_l\n","\n","def get_scidaily(last3):\n","  HEADERS = {'User-Agent': 'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148'}\n","  page = requests.get('https://www.sciencedaily.com/news/top/health/', headers=HEADERS)\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find('div', class_='hero tab-content')\n","  r2 = r1.find_all('div', class_='tab-pane')\n","\n","  l = []\n","  for i in range(len(r2)):\n","    d = {}\n","    timestamp = r2[i].find('span', class_='story-date').text\n","    if last3:\n","      if (str(today) + ',') not in timestamp and (str(yesterday) + ', ') not in timestamp and (str(yesterday2) + ', ') not in timestamp:\n","        break\n","    else:\n","      if (str(today) + ',') not in timestamp:\n","        break\n","\n","    meta_l = scrape_scidaily(r2[i])\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    if '...' in meta_l[2]:\n","      d['Description'] = meta_l[2][:-4]\n","    else:\n","      d['Description'] = meta_l[2]\n","    d['Source'] = 'Healthline'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"cellView":"form","id":"plA24O7jMG_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_cnn(text):\n","  meta_l = []\n","  meta = 'https://www.cnn.com/health'\n","\n","  spl_text = str(text.find('a', class_='container__link container_lead-plus-headlines__link')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('div', class_='container__headline container_lead-plus-headlines__headline').text.strip())\n","  meta_l.append('')\n","  return meta_l\n","\n","def get_cnn(last3=False):\n","  page = requests.get('https://www.cnn.com/health')\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find('div', class_='container__field-links container_lead-plus-headlines__field-links')\n","  r2 = r1.find_all('div', class_='card container__item container__item--type-section container_lead-plus-headlines__item container_lead-plus-headlines__item--type-section')  \n","\n","  l = []\n","  for i in range(1, len(r2)):\n","    d = {}\n","    meta_l = scrape_cnn(r2[i])\n","    timestamp = meta_l[0].split('/')[6]\n","\n","    if last3:\n","      if str(today) not in timestamp and str(yesterday) not in timestamp and str(yesterday2) not in timestamp:\n","        break\n","    else:\n","      if str(today) not in timestamp:\n","        break\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'CNN'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"id":"S6-AsxnSO9Ix","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_atlantic(text):\n","  meta_l = []\n","  meta = ''\n","\n","  spl_text = str(text).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('h2', class_='LandingRiver_title__4ibQ4').text)\n","  meta_l.append(text.find('p', class_='LandingRiver_dek__u9vaI').text)\n","  return meta_l\n","\n","def get_atlantic(last3=False):\n","  page = requests.get('https://www.theatlantic.com/health/')\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find('ul', class_='LandingRiver_ul__nRR9m')\n","  r2 = r1.find_all('li', class_='LandingRiver_li__Db7WD')\n","\n","  l = []\n","  if last3:\n","    limit = 6\n","  else: \n","    limit = 2\n","  for i in range(limit):\n","    d = {}    \n","    meta_l = scrape_atlantic(r2[i])\n","\n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'Atlantic'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"id":"OLyRqXMbUURx","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_time(text):\n","  meta_l = []\n","  meta = 'https://time.com'\n","\n","  spl_text = str(text.find('a')).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('h2', class_='headline').text.strip())\n","  if text.find('h3', class_='summary') != None:\n","    meta_l.append(text.find('h3', class_='summary').text.strip())\n","  else:\n","    meta_l.append('')\n","  return meta_l\n","\n","def get_time(last3=False):\n","  page = requests.get('https://time.com/section/health/')\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find_all('div', class_='taxonomy-tout')\n","\n","  l = []\n","  for i in range(len(r1)):\n","    d = {}\n","    if last3 and i >= 4:\n","      timestamp = r1[i].find('span', class_='byline').text\n","      if (str(today) + ',') not in timestamp and (str(yesterday) + ', ') not in timestamp and (str(yesterday2) + ', ') not in timestamp:\n","        break\n","    else:\n","      if i == 4:\n","        break\n","    \n","    meta_l = scrape_time(r1[i])\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'Time'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"id":"4QHEfdgeLku6","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","def scrape_reuters(text):\n","  meta_l = []\n","  meta = 'https://www.reuters.com'\n","\n","  spl_text = str(text).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    meta += spl_text[i]\n","\n","  meta_l.append(meta)\n","  meta_l.append(text.find('a', class_='text__text__1FZLe text__dark-grey__3Ml43 text__medium__1kbOh text__heading_5_and_half__3YluN heading__base__2T28j heading_5_half title__title__1Sa2a title').text)\n","  meta_l.append('')\n","  return meta_l\n","\n","def get_reuters():\n","  page = requests.get('https://www.reuters.com/business/healthcare-pharmaceuticals/')\n","  soup = BeautifulSoup(page.content, \"html.parser\")\n","  r1 = soup.find('div', class_='static-media-maximizer__hero__3I-8O')\n","  r2 = soup.find_all('li', class_='static-media-maximizer__card__3Ke0W static-media-maximizer__list__23N8R')\n","\n","  url = 'https://www.reuters.com'\n","  spl_text = str(r1).split('href=\"')[1]\n","  for i in range(len(spl_text)):\n","    if spl_text[i] == '\"':\n","      break\n","    url += spl_text[i]\n","\n","  l = []\n","  d = {}\n","  d['URL'] = url\n","  d['Headline'] = r1.find('a', class_='text__text__1FZLe text__dark-grey__3Ml43 text__medium__1kbOh text__heading_3__1kDhc heading__base__2T28j heading__heading_3__3aL54 title__title__1Sa2a hero-card__title__33EFM').text\n","  d['Description'] = r1.find('p', class_='text__text__1FZLe text__dark-grey__3Ml43 text__regular__2N1Xr text__body__yKS5U body__base__22dCE body__body__VgU9Q hero-card__description__2Ih7f').text\n","  d['Source'] = 'Reuters'\n","  l.append(d)\n","\n","  for i in range(len(r2)):\n","    d = {}\n","    meta_l = scrape_reuters(r2[i])\n","    \n","    d['URL'] = meta_l[0]\n","    d['Headline'] = meta_l[1]\n","    d['Description'] = meta_l[2]\n","    d['Source'] = 'Reuters'\n","    l.append(d)\n","    d = {}\n","\n","  return pd.DataFrame(l)"],"metadata":{"id":"C-_06S7rPJLH","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set flag to get news from last 3 days\n","last3 = True\n","dates = [today, yesterday, yesterday2]\n","\n","df = get_news(last3, dates).reset_index(drop=True)\n","df['Full_Text'] = df['Headline'] + '. ' + df['Description']\n","df"],"metadata":{"id":"Ar-H8p46zIjA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Topic Modeling"],"metadata":{"id":"5H2bXQ9WYaIe"}},{"cell_type":"code","source":["import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.corpora import Dictionary\n","from gensim.models.ldamodel import LdaModel\n","from gensim.models.coherencemodel import CoherenceModel\n","from gensim.models import Phrases\n","from gensim.models.phrases import Phraser\n","\n","import re\n","import spacy\n","import nltk\n","from nltk.tokenize import RegexpTokenizer\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","import pyLDAvis\n","from pyLDAvis.gensim_models import prepare\n","pyLDAvis.enable_notebook()"],"metadata":{"id":"jlehitWwVjr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts = df['Full_Text'].to_list()\n","\n","# pre-process text to remove extraneous symbols\n","texts = [re.sub('\\s+', ' ', text) for text in texts]\n","texts = [re.sub(\"\\'\", \"\", text) for text in texts]\n","texts = [re.sub(\"-\", \" \", text) for text in texts]\n","texts = [re.sub(\":\", \"\", text) for text in texts]"],"metadata":{"id":"saH89YO70lWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stops = set(stopwords.words('english'))\n","words = []\n","\n","# parse words from text and remove stopwords\n","for text in texts:\n","  words.append([word for word in simple_preprocess(str(text), deacc=True) if word not in stops])\n","\n","#nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n","nlp = spacy.load('en_core_web_sm')"],"metadata":{"id":"Ib4Yl2FR15hb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create sequences of 2 and 3 words\n","def make_ngrams(texts):\n","  bigram = Phrases(words, min_count=5, threshold=100)\n","  bigram_mod = Phraser(bigram)\n","  bigrams = [bigram_mod[text] for text in texts]\n","\n","  trigram = Phrases(bigram[words], threshold=100)\n","  trigram_mod = Phraser(trigram)  \n","  trigrams = [trigram_mod[bigram_mod[text]] for text in texts]\n","  return bigrams, trigrams\n","\n","# break down words to root forms\n","def lemmatize(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","  ntexts = []\n","  for text in texts:\n","    doc = nlp(\" \".join(text)) \n","    ntexts.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n","  return ntexts"],"metadata":{"id":"CbDHSiBp2coD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bigram_texts = make_ngrams(words)[0]\n","corpus = lemmatize(bigram_texts)\n","\n","# get term frequency vector\n","id2word = Dictionary(corpus)\n","tdf = [id2word.doc2bow(text) for text in corpus]"],"metadata":{"id":"SBWnZsIe4g5R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fit LDA model on corpus with variable number of topics\n","coh_values = []\n","models = []\n","for num_topics in range(2, 21):\n","  lda = LdaModel(corpus=tdf, id2word=id2word, num_topics=num_topics, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n","  models.append(lda)\n","  coh_model = CoherenceModel(model=lda, texts=corpus, dictionary=id2word, coherence='c_v')\n","  coh_values.append(coh_model.get_coherence())"],"metadata":{"id":"lyW5V7wJtqm8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select best LDA model based on coherence score\n","idx = coh_values.index(max(coh_values))\n","num_topics = idx + 2\n","best_lda = models[idx]"],"metadata":{"id":"6tCnnbHmLxfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Perplexity: {best_lda.log_perplexity(tdf)}\")\n","print(f\"Coherence Score: {coh_values[idx]}\")\n","best_lda.print_topics()"],"metadata":{"id":"KiPwdyBft-EB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize topic distribution\n","vis = prepare(best_lda, tdf, id2word)\n","vis"],"metadata":{"id":"wdiW38xp1tJp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create DataFrame to find each article's dominant topic\n","l = []\n","for i, r in enumerate(best_lda[tdf]):\n","  d = {}\n","  dom_topic = r[0][0][0]\n","  kwords = []\n","  for x in best_lda.show_topic(dom_topic):\n","    kwords.append(x[0])\n","\n","  d['Dominant_Topic'] = dom_topic\n","  d['Dominant_Topic_Keywords'] = ', '.join(kwords)\n","  d['Headline'] = df.iloc[i]['Headline']\n","  d['URL'] = df.iloc[i]['URL']\n","  l.append(d)\n","\n","ddf = pd.DataFrame(l)\n","ddf"],"metadata":{"id":"e8N3aC6yzh3C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Keyword Extraction"],"metadata":{"id":"IZYmor-SYWTi"}},{"cell_type":"code","source":["from multi_rake import Rake\n","from keybert import KeyBERT\n","\n","from collections import Counter\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"eCYPUS6FaH3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","texts = df['Full_Text'].to_list()\n","\n","rake = Rake()\n","keybert = KeyBERT(model='all-mpnet-base-v2')"],"metadata":{"id":"RfPKfsWvbgqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract keywords from corpus using separate approaches\n","rake_kwords = []\n","bert_kwords = []\n","for text in texts:\n","  rake_text = rake.apply(text)\n","  if rake_text != []:\n","    rake_kwords.append(rake_text)\n","  else:\n","    rake_kwords.append('')\n","  bert_kwords.append(list(dict(keybert.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', use_mmr=True, top_n=10)).keys()))"],"metadata":{"id":"JRq7XMcU7utY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create DataFrame to get each article's individual keywords, verified by both approaches\n","l = []\n","for i in range(len(bert_kwords)):\n","  d = {'Article_Keywords': []}\n","  d['Headline'] = df.iloc[i]['Headline']\n","  d['URL'] = df.iloc[i]['URL']\n","\n","  for kword in bert_kwords[i]:\n","    token = nlp(kword)\n","    for kword2 in rake_kwords[i]:\n","      # add word as keyword if found by both approaches\n","      if kword == kword2[0]:\n","        d['Article_Keywords'].append(kword)\n","      # add word as keyword if semantically similar to any word found by other approach\n","      else:\n","        token2 = nlp(kword2[0])\n","        score = token.similarity(token2)\n","        if score >= 0.8:\n","          d['Article_Keywords'].append(kword)\n","\n","  l.append(d)\n","\n","ddf2 = pd.DataFrame(l)\n","ddf2"],"metadata":{"id":"lIJ-Hm1W1wL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_kwords = []\n","for i, r in ddf2.iterrows():\n","  all_kwords.extend(r['Article_Keywords'])\n","\n","counter = Counter(all_kwords)"],"metadata":{"id":"E78Mh5dGPKHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot top k keywords for days' news articles\n","k = 20\n","topk_kwords = counter.most_common(k)\n","\n","x = []\n","y = []\n","for kword in topk_kwords:\n","  x.append(kword[0])\n","  y.append(kword[1])\n","\n","if last3:\n","  plt.figure(figsize=(k*1.5, k*0.75))\n","  plt.title(f\"Top {k} Keywords of the Last 3 Days\", fontdict={'fontsize': k*1.5})\n","  plt.tight_layout()\n","  _ = sns.barplot(x=x, y=y)\n","else:\n","  plt.figure(figsize=(k*1.5, k*0.75))\n","  plt.title(f\"Top {k} Keywords of the Day\", fontdict={'fontsize': k*1.5})\n","  plt.tight_layout()\n","  _ = sns.barplot(x=x, y=y)"],"metadata":{"id":"GnkRuaV5Rp3k"},"execution_count":null,"outputs":[]}]}